
@article{noauthor_notitle_nodate,
}

@article{schwarz_corpus-based_2007,
	title = {Corpus-Based Concatenative Synthesis},
	volume = {24},
	issn = {1053-5888},
	url = {http://ieeexplore.ieee.org/document/4117932/},
	doi = {10.1109/MSP.2007.323274},
	pages = {92--104},
	number = {2},
	journaltitle = {{IEEE} Signal Processing Magazine},
	shortjournal = {{IEEE} Signal Process. Mag.},
	author = {Schwarz, D.},
	urldate = {2023-01-16},
	date = {2007-03},
}

@inproceedings{schwarz_what_2008,
	location = {Belfast, United Kingdom},
	title = {What Next? Continuation in Real-Time Corpus-Based Concatenative Synthesis},
	url = {https://hal.archives-ouvertes.fr/hal-01161402},
	pages = {1--1},
	booktitle = {International Computer Music Conference ({ICMC})},
	author = {Schwarz, Diemo and Cadars, Sylvain and Schnell, Norbert},
	date = {2008-08},
	keywords = {concatenative synthesis, content-based selection, continuation prediction, corpus-based synthesis, data-driven synthesis, database, drum-loops, expectation modeling, Informatique musicale, interaction, Real-time, sequence, sound synthesis, unit selection},
	annotation = { cote interne {IRCAM}: Schwarz08c},
}

@article{tremblay_enabling_2021,
	title = {Enabling Programmatic Data Mining as Musicking: The Fluid Corpus Manipulation Toolkit},
	volume = {45},
	issn = {0148-9267},
	url = {https://doi.org/10.1162/comj_a_00600},
	doi = {10.1162/comj_a_00600},
	abstract = {This article presents a new software toolbox to enable programmatic mining of sound banks for musicking and musicking-driven research. The toolbox is available for three popular creative coding environments currently used by “techno-fluent” musicians. The article describes the design rationale and functionality of the toolbox and its ecosystem, then the development methodology—several versions of the toolbox have been seeded to early adopters who have, in turn, contributed to the design. Examples of these early usages are presented, and we describe some observed musical affordances of the proposed approach to the exploration and manipulation of music corpora, as well as the main roadblocks encountered. We finally reflect on a few emerging themes for the next steps in building a community around critical programmatic mining of sound banks.},
	pages = {9--23},
	number = {2},
	journaltitle = {Computer Music Journal},
	shortjournal = {Computer Music Journal},
	author = {Tremblay, Pierre Alexandre and Roma, Gerard and Green, Owen},
	urldate = {2023-01-16},
	date = {2021-06-01},
}

@misc{pasini_musika_2022,
	title = {Musika! Fast Infinite Waveform Music Generation},
	url = {http://arxiv.org/abs/2208.08706},
	abstract = {Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer {GPU}, and that allows for much faster than real-time generation of music of arbitrary length on a consumer {CPU}. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network ({GAN}) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a {GAN} can be trained on a new music domain with a single {GPU} in a matter of hours.},
	number = {{arXiv}:2208.08706},
	publisher = {{arXiv}},
	author = {Pasini, Marco and Schlüter, Jan},
	urldate = {2023-01-16},
	date = {2022-08-18},
	eprinttype = {arxiv},
	eprint = {2208.08706 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annotation = {Comment: Accepted at {ISMIR} 2022},
	file = {arXiv Fulltext PDF:files/16/Pasini and Schlüter - 2022 - Musika! Fast Infinite Waveform Music Generation.pdf:application/pdf;arXiv.org Snapshot:files/17/2208.html:text/html},
}

@misc{dhariwal_jukebox_2020,
	title = {Jukebox: A Generative Model for Music},
	url = {http://arxiv.org/abs/2005.00341},
	shorttitle = {Jukebox},
	abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale {VQ}-{VAE} to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
	number = {{arXiv}:2005.00341},
	publisher = {{arXiv}},
	author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
	urldate = {2023-01-16},
	date = {2020-04-30},
	eprinttype = {arxiv},
	eprint = {2005.00341 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:files/19/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf:application/pdf;arXiv.org Snapshot:files/20/2005.html:text/html},
}

@online{mcleavey_payne_musenet_nodate,
	title = {{MuseNet}},
	url = {https://openai.com/blog/musenet/},
	abstract = {We’ve created {MuseNet}, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. {MuseNet} was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of {MIDI} files. {MuseNet} uses the same general-purpose unsupervised technology as {GPT}-2, a large-scale transformer model trained to predict the next token in a sequence, whether audio or text.},
	titleaddon = {{OpenAI} Blog},
	author = {{McLeavey} Payne, Christine},
}
