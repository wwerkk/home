### Introduction
#### Project overview
This is an attempt of implementing a software tool capable of generating creative and coherent variations/extensions of analysed audio signal using concatenative synthesis, enhanced by generative language modelling.
Its motivation is to investigate the combination of methods which, while not covered by recent audio related deep learning projects, hints at the possibility of generating high quality output without much computational power required, greatly improving the accessibility of recent deep learning developments for creative sound practice. 
Concatenative synthesis in its basic form is completely context-agnostic, therefore what is being sought in this project is a method for generating context-aware trajectories across the space of sound corpora. 
This is a task suitable for neural network architecture capable of sequence generation via predicting the next possible token based on previous ones, which is called a *language model*. Intuitively, this would mainly suggest its ability to process text, though it can be efficiently generalized to model any kind of sequence data, sound and music not being exceptions [^8].
The fundamental idea of this project is to granulate a given audio signal, learn its statistical *latent space* and generate a sequence of arbitrary length based on that model.
#### Background research
Agiven input signal can be easily split at transients or regular intervals into chunks called *grains* which can then be played back, in a method described as granular re-synthesis or *granulation*[^9]. An extended approach to it undertakes an analysis of each grain content, extracting multiple high level features (also called descriptors) values of which can then serve as N-dimensional coordinates, organising the chunks of what was initially an audio sequence into a multi-dimensional space open for creative exploration[^10].

![[catart.png|400]] 
Fig. 1 - [*2D display of catart.lcd5*](http://catart.concatenative.net/)
Concatenative audio synthesis has a long history of introducting ML methods to creative sound practices. Corpus-Based Concatenative Synthesis toolboxes such as IRCAM's CataRT[^1] and FluCoMa[^2] developed at University of Huddersfield both provide advanced tools for audio decomposition, analysis and re-synthesis. In case of CataRT, various approaches to sequentiality have been attempted, including predicting via hidden Markov models and weighted context-aware methods working in real-time[^3]. A decade later, with increasing popularity of the Deep Learning branch of Machine Learning research, a successful attempt has been made to replace the calculated descriptors of audio grains with learned latent representations using variational auto-encoders (VAEs). Neural Granular Sound Synthesis[^3] which was developed at IRCAM, had been followed in 2021 by a a real-time implementation of an audio VAE - RAVE[^4] which also uses a Generative Adversarial Network (GAN) to model trajectories between latent representations of signal frames, enabling non-conditioned generation. While the signal generation runs efficiently (and, importantly, faster than real-time) on an average consumer CPU, training a generative model with RAVE still takes several days on a high-end GPU. More so, it actually consists of two models - one responsible for encoding and decoding latent representations, the other for modelling trajectories across the former model's latent space.
Two years before, OpenAI released MuseNet[^5] and then JukeBox[^6] transformer-based networks, which apply generative language modelling to sequences of MIDI notes extracted from fed samples. In case of the latter, generated sequences are then fed into WaveNet[^7], which calculates the output raw audio on per-sample basis. Being high quality, this kind of pipeline proves to be very computationally intensive during inference, while resources for fitting a model of such large scale are beyond the reach of an individual. 