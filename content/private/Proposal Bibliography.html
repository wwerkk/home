<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>Bibliography</title>
</head>
<body>
<div class="csl-bib-body" style="line-height: 1.35; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Caillon, Antoine, and Philippe Esling. ‘RAVE: A Variational Autoencoder for Fast and High-Quality Neural Audio Synthesis’. arXiv, 15 December 2021. <a href="http://arxiv.org/abs/2111.05011">http://arxiv.org/abs/2111.05011</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=RAVE%3A%20A%20variational%20autoencoder%20for%20fast%20and%20high-quality%20neural%20audio%20synthesis&amp;rft.description=Deep%20generative%20models%20applied%20to%20audio%20have%20improved%20by%20a%20large%20margin%20the%20state-of-the-art%20in%20many%20speech%20and%20music%20related%20tasks.%20However%2C%20as%20raw%20waveform%20modelling%20remains%20an%20inherently%20difficult%20task%2C%20audio%20generative%20models%20are%20either%20computationally%20intensive%2C%20rely%20on%20low%20sampling%20rates%2C%20are%20complicated%20to%20control%20or%20restrict%20the%20nature%20of%20possible%20signals.%20Among%20those%20models%2C%20Variational%20AutoEncoders%20(VAE)%20give%20control%20over%20the%20generation%20by%20exposing%20latent%20variables%2C%20although%20they%20usually%20suffer%20from%20low%20synthesis%20quality.%20In%20this%20paper%2C%20we%20introduce%20a%20Realtime%20Audio%20Variational%20autoEncoder%20(RAVE)%20allowing%20both%20fast%20and%20high-quality%20audio%20waveform%20synthesis.%20We%20introduce%20a%20novel%20two-stage%20training%20procedure%2C%20namely%20representation%20learning%20and%20adversarial%20fine-tuning.%20We%20show%20that%20using%20a%20post-training%20analysis%20of%20the%20latent%20space%20allows%20a%20direct%20control%20between%20the%20reconstruction%20fidelity%20and%20the%20representation%20compactness.%20By%20leveraging%20a%20multi-band%20decomposition%20of%20the%20raw%20waveform%2C%20we%20show%20that%20our%20model%20is%20the%20first%20able%20to%20generate%2048kHz%20audio%20signals%2C%20while%20simultaneously%20running%2020%20times%20faster%20than%20real-time%20on%20a%20standard%20laptop%20CPU.%20We%20evaluate%20synthesis%20quality%20using%20both%20quantitative%20and%20qualitative%20subjective%20experiments%20and%20show%20the%20superiority%20of%20our%20approach%20compared%20to%20existing%20models.%20Finally%2C%20we%20present%20applications%20of%20our%20model%20for%20timbre%20transfer%20and%20signal%20compression.%20All%20of%20our%20source%20code%20and%20audio%20examples%20are%20publicly%20available.&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F2111.05011&amp;rft.aufirst=Antoine&amp;rft.aulast=Caillon&amp;rft.au=Antoine%20Caillon&amp;rft.au=Philippe%20Esling&amp;rft.date=2021-12-15"></span>
  <div class="csl-entry">Dhariwal, Prafulla, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. ‘Jukebox: A Generative Model for Music’. arXiv, 30 April 2020. <a href="http://arxiv.org/abs/2005.00341">http://arxiv.org/abs/2005.00341</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Jukebox%3A%20A%20Generative%20Model%20for%20Music&amp;rft.description=We%20introduce%20Jukebox%2C%20a%20model%20that%20generates%20music%20with%20singing%20in%20the%20raw%20audio%20domain.%20We%20tackle%20the%20long%20context%20of%20raw%20audio%20using%20a%20multi-scale%20VQ-VAE%20to%20compress%20it%20to%20discrete%20codes%2C%20and%20modeling%20those%20using%20autoregressive%20Transformers.%20We%20show%20that%20the%20combined%20model%20at%20scale%20can%20generate%20high-fidelity%20and%20diverse%20songs%20with%20coherence%20up%20to%20multiple%20minutes.%20We%20can%20condition%20on%20artist%20and%20genre%20to%20steer%20the%20musical%20and%20vocal%20style%2C%20and%20on%20unaligned%20lyrics%20to%20make%20the%20singing%20more%20controllable.%20We%20are%20releasing%20thousands%20of%20non%20cherry-picked%20samples%20at%20https%3A%2F%2Fjukebox.openai.com%2C%20along%20with%20model%20weights%20and%20code%20at%20https%3A%2F%2Fgithub.com%2Fopenai%2Fjukebox&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F2005.00341&amp;rft.aufirst=Prafulla&amp;rft.aulast=Dhariwal&amp;rft.au=Prafulla%20Dhariwal&amp;rft.au=Heewoo%20Jun&amp;rft.au=Christine%20Payne&amp;rft.au=Jong%20Wook%20Kim&amp;rft.au=Alec%20Radford&amp;rft.au=Ilya%20Sutskever&amp;rft.date=2020-04-30"></span>
  <div class="csl-entry">McLeavey Payne, Christine. ‘MuseNet’. <i>OpenAI Blog</i> (blog), n.d. <a href="https://openai.com/blog/musenet/">https://openai.com/blog/musenet/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=MuseNet&amp;rft.description=We%E2%80%99ve%20created%20MuseNet%2C%20a%20deep%20neural%20network%20that%20can%20generate%204-minute%20musical%20compositions%20with%2010%20different%20instruments%2C%20and%20can%20combine%20styles%20from%20country%20to%20Mozart%20to%20the%20Beatles.%20MuseNet%20was%20not%20explicitly%20programmed%20with%20our%20understanding%20of%20music%2C%20but%20instead%20discovered%20patterns%20of%20harmony%2C%20rhythm%2C%20and%20style%20by%20learning%20to%20predict%20the%20next%20token%20in%20hundreds%20of%20thousands%20of%20MIDI%20files.%20MuseNet%20uses%20the%20same%20general-purpose%20unsupervised%20technology%20as%20GPT-2%2C%20a%20large-scale%20transformer%20model%20trained%20to%20predict%20the%20next%20token%20in%20a%20sequence%2C%20whether%20audio%20or%20text.&amp;rft.identifier=https%3A%2F%2Fopenai.com%2Fblog%2Fmusenet%2F&amp;rft.aufirst=Christine&amp;rft.aulast=McLeavey%20Payne&amp;rft.au=Christine%20McLeavey%20Payne"></span>
  <div class="csl-entry">Pasini, Marco, and Jan Schlüter. ‘Musika! Fast Infinite Waveform Music Generation’. arXiv, 18 August 2022. <a href="http://arxiv.org/abs/2208.08706">http://arxiv.org/abs/2208.08706</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=preprint&amp;rft.title=Musika!%20Fast%20Infinite%20Waveform%20Music%20Generation&amp;rft.description=Fast%20and%20user-controllable%20music%20generation%20could%20enable%20novel%20ways%20of%20composing%20or%20performing%20music.%20However%2C%20state-of-the-art%20music%20generation%20systems%20require%20large%20amounts%20of%20data%20and%20computational%20resources%20for%20training%2C%20and%20are%20slow%20at%20inference.%20This%20makes%20them%20impractical%20for%20real-time%20interactive%20use.%20In%20this%20work%2C%20we%20introduce%20Musika%2C%20a%20music%20generation%20system%20that%20can%20be%20trained%20on%20hundreds%20of%20hours%20of%20music%20using%20a%20single%20consumer%20GPU%2C%20and%20that%20allows%20for%20much%20faster%20than%20real-time%20generation%20of%20music%20of%20arbitrary%20length%20on%20a%20consumer%20CPU.%20We%20achieve%20this%20by%20first%20learning%20a%20compact%20invertible%20representation%20of%20spectrogram%20magnitudes%20and%20phases%20with%20adversarial%20autoencoders%2C%20then%20training%20a%20Generative%20Adversarial%20Network%20(GAN)%20on%20this%20representation%20for%20a%20particular%20music%20domain.%20A%20latent%20coordinate%20system%20enables%20generating%20arbitrarily%20long%20sequences%20of%20excerpts%20in%20parallel%2C%20while%20a%20global%20context%20vector%20allows%20the%20music%20to%20remain%20stylistically%20coherent%20through%20time.%20We%20perform%20quantitative%20evaluations%20to%20assess%20the%20quality%20of%20the%20generated%20samples%20and%20showcase%20options%20for%20user%20control%20in%20piano%20and%20techno%20music%20generation.%20We%20release%20the%20source%20code%20and%20pretrained%20autoencoder%20weights%20at%20github.com%2Fmarcoppasini%2Fmusika%2C%20such%20that%20a%20GAN%20can%20be%20trained%20on%20a%20new%20music%20domain%20with%20a%20single%20GPU%20in%20a%20matter%20of%20hours.&amp;rft.identifier=http%3A%2F%2Farxiv.org%2Fabs%2F2208.08706&amp;rft.aufirst=Marco&amp;rft.aulast=Pasini&amp;rft.au=Marco%20Pasini&amp;rft.au=Jan%20Schl%C3%BCter&amp;rft.date=2022-08-18"></span>
  <div class="csl-entry">Schwarz, D. ‘Corpus-Based Concatenative Synthesis’. <i>IEEE Signal Processing Magazine</i> 24, no. 2 (March 2007): 92–104. <a href="https://doi.org/10.1109/MSP.2007.323274">https://doi.org/10.1109/MSP.2007.323274</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FMSP.2007.323274&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Corpus-Based%20Concatenative%20Synthesis&amp;rft.jtitle=IEEE%20Signal%20Processing%20Magazine&amp;rft.stitle=IEEE%20Signal%20Process.%20Mag.&amp;rft.volume=24&amp;rft.issue=2&amp;rft.aufirst=D.&amp;rft.aulast=Schwarz&amp;rft.au=D.%20Schwarz&amp;rft.date=2007-03&amp;rft.pages=92-104&amp;rft.spage=92&amp;rft.epage=104&amp;rft.issn=1053-5888"></span>
  <div class="csl-entry">Schwarz, Diemo, Sylvain Cadars, and Norbert Schnell. ‘What Next? Continuation in Real-Time Corpus-Based Concatenative Synthesis’. In <i>International Computer Music Conference (ICMC)</i>, 1–1. Belfast, United Kingdom, 2008. <a href="https://hal.archives-ouvertes.fr/hal-01161402">https://hal.archives-ouvertes.fr/hal-01161402</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=What%20Next%3F%20Continuation%20in%20Real-Time%20Corpus-Based%20Concatenative%20Synthesis&amp;rft.btitle=International%20Computer%20Music%20Conference%20(ICMC)&amp;rft.place=Belfast%2C%20United%20Kingdom&amp;rft.aufirst=Diemo&amp;rft.aulast=Schwarz&amp;rft.au=Diemo%20Schwarz&amp;rft.au=Sylvain%20Cadars&amp;rft.au=Norbert%20Schnell&amp;rft.date=2008-08&amp;rft.pages=1-1&amp;rft.spage=1&amp;rft.epage=1"></span>
  <div class="csl-entry">Tremblay, Pierre Alexandre, Gerard Roma, and Owen Green. ‘Enabling Programmatic Data Mining as Musicking: The Fluid Corpus Manipulation Toolkit’. <i>Computer Music Journal</i> 45, no. 2 (1 June 2021): 9–23. <a href="https://doi.org/10.1162/comj_a_00600">https://doi.org/10.1162/comj_a_00600</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1162%2Fcomj_a_00600&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Enabling%20Programmatic%20Data%20Mining%20as%20Musicking%3A%20The%20Fluid%20Corpus%20Manipulation%20Toolkit&amp;rft.jtitle=Computer%20Music%20Journal&amp;rft.stitle=Computer%20Music%20Journal&amp;rft.volume=45&amp;rft.issue=2&amp;rft.aufirst=Pierre%20Alexandre&amp;rft.aulast=Tremblay&amp;rft.au=Pierre%20Alexandre%20Tremblay&amp;rft.au=Gerard%20Roma&amp;rft.au=Owen%20Green&amp;rft.date=2021-06-01&amp;rft.pages=9-23&amp;rft.spage=9&amp;rft.epage=23&amp;rft.issn=0148-9267"></span>
</div></body>
</html>
